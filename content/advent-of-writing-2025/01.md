---
title: "Getting started with Runpod Flash"
slug: "01-getting-started-with-runpod-flash"
date: 2025-12-01T00:00:00+01:00
draft: false
type: "subpage"
img: "20251201_day_01"
introimg: "/shows/intro/20251201_day_01.jpg"
---

[Runpod Flash](https://github.com/runpod/tetra-rp) (we'll just call it "flash" from here) lets you write your code locally while it provisions GPU / CPU resources for you on Runpod on the fly.

This makes it possible to iterate very fast with any idea you might have, for example serving a model as an API, without worrying about Docker and setting up endpoints for testing.

All of that is taken care of by flash with a couple of simple commands.

## Install flash cli

I'll be using [uv](https://github.com/astral-sh/uv) for this tutorial. If you don't have it installed yet, please [install it first](https://docs.astral.sh/uv/getting-started/installation/) (it's fast!).

In order to use flash everywhere, I recommend installing it globally:

```bash
uv tool install tetra_rp
```

{{< note type="info" >}}
The package is still named `tetra_rp` (our internal name during development), but will be renamed soon. Sorry for any confusion :D
{{< /note >}}

You can check if the installation went fine by running this:

```bash
flash --version
```

This should output something like this: `Runpod Flash CLI v0.18.0`

## Create a project

The next thing we have to do is to create a new project:

```bash
flash init hello-world
cd hello-world
```

This creates the following structure:

```
hello-world/
├── main.py                # FastAPI entry point
├── workers/
│   ├── gpu/
│   │   ├── __init__.py    # FastAPI router
│   │   └── endpoint.py    # @remote decorated function
│   └── cpu/
│       ├── __init__.py
│       └── endpoint.py
├── .env.example
├── requirements.txt
└── README.md
```

### Setup

Create a virtual environment and install dependencies:

```bash
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

Copy the `.env.example` file:

```bash
cp .env.example .env
```

Edit `.env` and set your [Runpod API key](https://runpod.io/console/user/settings)

```
RUNPOD_API_KEY=your_key_here
```

Great, now the project is setup and we can take a look on how it works.

## @remote decorator

The `@remote` decorator tells flash that this function should run on Runpod instead of in your local process.

It does **not** call the function by itself – it only changes _where_ it runs when something (let's keep this in mind, might come in handy later) calls it.

Here is a high-level overview of the flow:

<div class="mermaid">
sequenceDiagram
    participant Client
    participant Local
    participant Runpod
    participant Worker
    Client->>Local: POST /gpu/hello
    Note over Local: Intercepts call
    Local->>Runpod: Submit Job
    Runpod->>Worker: Provision and Execute
    Worker->>Runpod: Return Result
    Runpod->>Local: Job Completed
    Local->>Client: 200 OK
</div>

### The endpoint code

The default code generated by `flash init` is a bit complex because it handles error checking and detailed stats.

Let's replace `workers/gpu/endpoint.py` with a simplified version that is easier to understand, but keeps the same function name `gpu_hello` so everything else keeps working:

```python
from tetra_rp import remote, LiveServerless, GpuGroup

# Configuration for your serverless endpoint on Runpod
gpu_config = LiveServerless(
    name="hello-world-gpu",  # Unique name for this endpoint
    gpus=[GpuGroup.ANY],     # Use any available GPU
    workersMin=0,            # Scale to zero when idle
    workersMax=3,            # Max concurrent workers
    idleTimeout=5,           # Seconds before worker shuts down when no new request comes in
)

# List of packages to install on the worker
dependencies = ["torch"]

@remote(
    resource_config=gpu_config,
    dependencies=dependencies
)
async def gpu_hello(input_data: dict) -> dict:
    # import "dependencies" inside the function
    import torch

    # Get the message from the input
    name = input_data.get("message", "World")

    # your code runs on an actual GPU
    greeting = f"Hello {name}, from a GPU!"
    gpu_name = torch.cuda.get_device_name(0)
    cuda_available = torch.cuda.is_available()

    # you decide what to return
    return {
        "greeting": greeting,
        "gpu": gpu_name,
        "cuda_available": cuda_available,
    }
```

{{< note type="tip" >}}
Multiple functions can share the same endpoint if they use the same `LiveServerless` config. Each function call becomes a separate job on that endpoint. If you want separate endpoints for different functions, create separate `LiveServerless` configs with different names.
{{< /note >}}

Flash also sets up a FastAPI endpoint for you in `workers/gpu/__init__.py`. You can take a look if you want, but it just exposes our function via HTTP.

## running flash

Now that our project is set up, we can start the development server. This command wraps `uvicorn` and handles the connection between your local code and the remote workers:

```bash
flash run
```

This starts the local development server. You should see output similar to this:

```text
Starting Flash Server
Entry point: main:app
Server: http://localhost:8888
Auto-reload: enabled

Press CTRL+C to stop

INFO:     Will watch for changes in these directories: ['/path/to/hello-world']
INFO:     Uvicorn running on http://localhost:8888 (Press CTRL+C to quit)
INFO:     Started server process [86759]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

Open `http://localhost:8888/docs` in your browser to access the Swagger UI.

## Test the endpoint

In Swagger UI, expand `/gpu/hello`, click "Try it out", enter a name, and hit "Execute".

Or use curl:

```bash
curl -X POST http://localhost:8888/gpu/hello \
    -H "Content-Type: application/json" \
    -d '{"message": "NERDDISCO"}'
```

Your terminal will show logs similar to this:

```text
2025-12-01 17:31:43,276 | INFO  | Created endpoint: s8w0gpafvl6707 - hello-world-gpu-fb
2025-12-01 17:31:44,040 | INFO  | LiveServerless:s8w0gpafvl6707 | Started Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2
2025-12-01 17:31:44,090 | INFO  | Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2 | Status: IN_QUEUE
2025-12-01 17:31:44,479 | INFO  | Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2 | .
2025-12-01 17:31:45,757 | INFO  | Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2 | ..
...
2025-12-01 17:33:38,591 | INFO  | Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2 | Status: COMPLETED
2025-12-01 17:33:38,618 | INFO  | Worker:na8dbgu8htou5e | Delay Time: 109571 ms
2025-12-01 17:33:38,619 | INFO  | Worker:na8dbgu8htou5e | Execution Time: 910 ms
dependency_installer.py:35 Installing Python dependencies: ['torch']
INFO:     127.0.0.1:52355 - "POST /gpu/hello HTTP/1.1" 200 OK
```

{{< note type="info" >}}
The first request takes ~60-90 seconds because flash has to provision a GPU worker and install dependencies like `torch`. Subsequent requests are much faster (~1-2s) as long as the worker stays warm. The `idleTimeout` setting controls how long workers stay alive after completing work.
{{< /note >}}

### What happens under the hood

When you send a request to `/gpu/hello` (via Swagger UI or curl), FastAPI calls the `hello` route, which calls `gpu_hello()` in `endpoint.py`.

Because `gpu_hello` is decorated with `@remote`, flash does the following instead of running it locally:

1. Creates (or reuses) a serverless endpoint on Runpod for `hello-world-gpu`
2. Serializes the function input (`input_data`) and submits a job to that endpoint
3. Provisions a GPU worker if none is available yet — this is the cold start you see as dots in the logs
4. Installs the dependencies you listed in `dependencies=["torch"]` on that worker
5. Executes `gpu_hello` on the GPU and captures the return value
6. Sends the result back to your FastAPI app, which returns it as the HTTP response

### Expected output

```json
{
  "greeting": "Hello NERDDISCO, from a GPU!",
  "gpu": "NVIDIA GeForce RTX 4090",
  "cuda_available": true
}
```

## Stopping & Cleanup

To stop the local server, just press `Ctrl+C`.

The remote worker will automatically shut down after 5 seconds of inactivity (thanks to `idleTimeout=5` in our config), so you won't keep paying for the GPU.

To fully remove the endpoint definition, go to your [Runpod Console > Serverless](https://runpod.io/console/serverless).

## Alpha status

Flash is in alpha. Things may change or break. I'll do my best to update this article when that happens.

- `flash run` runs the **HTTP server** on your machine, which acts as a bridge to the remote workers
- Endpoints persist in your Runpod account until manually deleted
- Local `.env` vars don't auto-forward to workers (use `env={}` in config)

Please report issues on [GitHub](https://github.com/runpod/tetra-rp/issues).
