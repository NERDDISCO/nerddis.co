---
title: "Getting started with Runpod Flash"
slug: "01-getting-started-with-runpod-flash"
date: 2025-12-01T00:00:00+01:00
draft: false
type: "subpage"
img: "20251201_day_01"
introimg: "/shows/intro/20251201_day_01.jpg"
---

[Runpod Flash](https://github.com/runpod/tetra-rp) (we'll just call it "flash" from here) lets you write your code locally while it provisions GPU / CPU resources for you on Runpod on the fly.

This makes it possible to iterate very fast with any idea you might have, for example serving a model as an API, without worrying about Docker and setting up endpoints for testing.

All of that is taken care of by flash with a couple of simple commands.

## Install flash cli

In order to use flash everywhere, I recommend to install it globally:

```bash
uv tool install tetra_rp
```

Note: The package is still named `tetra_rp` (our internal name during development), but will be renamed soon. Sorry for any confusion :D

You can check if the installation went fine by running this:

```bash
flash --version
```

This should output something like this: `Runpod Flash CLI v0.18.0`

## Create a project

The next thing we have to do is to create a new project:

```bash
flash init hello-world
cd hello-world
```

This creates the following structure:

```
hello-world/
├── main.py                # FastAPI entry point
├── workers/
│   ├── gpu/
│   │   ├── __init__.py    # FastAPI router
│   │   └── endpoint.py    # @remote decorated function
│   └── cpu/
│       ├── __init__.py
│       └── endpoint.py
├── .env.example
├── requirements.txt
└── README.md
```

### Setup

Create a virtual environment and install dependencies:

```bash
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

Copy the `.example.env` file:

```bash
cp .env.example .env
```

Edit `.env` and set your [Runpod API key](https://runpod.io/console/user/settings)

```
RUNPOD_API_KEY=your_key_here
```

Great, now the project is setup and we can take a look on how it works.

## @remote decorator

The `@remote` decorator tells flash that this function should run on Runpod instead of in your local process.

It does **not** call the function by itself – it only changes _where_ it runs when something (lets keep this in mind, might come in handy later) calls it.

Let's replace the content of `workers/gpu/endpoint.py` with:

```python
from tetra_rp import remote, LiveServerless

# Configuration for your serverless endpoint on Runpod
gpu_config = LiveServerless(
    name="hello-world-gpu",  # Unique name for this endpoint
    workersMin=0,            # Scale to zero when idle
    workersMax=3,            # Max concurrent workers
    idleTimeout=5,           # Seconds before worker shuts down
)

# List of packages, which are installed on the worker
dependencies = ["torch"]

# decorator: this function should run on Runpod
@remote(
    resource_config=gpu_config,
    dependencies=dependencies,
)
async def say_hello(name: str) -> dict:
    # import "dependencies"
    import torch

    # your code runs on an actual GPU
    greeting = f"Hello {name}, from a GPU!"
    gpu_name = torch.cuda.get_device_name(0)
    cuda_available = torch.cuda.is_available()

    # you decide what to return
    return {
        "greeting": greeting,
        "gpu": gpu_name,
        "cuda_available": cuda_available,
    }
```

### api

In this hello-world, that “something” is the FastAPI route at `/gpu/hello` that we’ll wire up next.

And update `workers/gpu/__init__.py` to use the new function (this is the FastAPI side that actually calls `say_hello` when someone hits `/gpu/hello`):

```python
from fastapi import APIRouter
from pydantic import BaseModel

from .endpoint import say_hello

gpu_router = APIRouter()

class HelloRequest(BaseModel):
    name: str = "World"

@gpu_router.post("/hello")
async def hello(request: HelloRequest):
    return await say_hello(request.name)
```

### How the `/gpu/hello` route is built

If you now look at `main.py`, you'll see how the full route is built:

- `app.include_router(gpu_router, prefix="/gpu", ...)` mounts the GPU router at `/gpu`
- the router itself defines `@gpu_router.post("/hello")`

Combined, this gives you the final path: `/gpu/hello`. If you change the `prefix` or the `@gpu_router.post(...)` path, the URL will change accordingly. The folder being called `gpu` is just a convention here – the route comes from these two strings, not from the directory name.

## Run the server

```bash
flash run
```

Open `http://localhost:8888/docs` in your browser to access the Swagger UI.

## Test the endpoint

In Swagger UI, expand `/gpu/hello`, click "Try it out", enter a name, and hit "Execute".

Or use curl:

```bash
curl -X POST http://localhost:8888/gpu/hello \
    -H "Content-Type: application/json" \
    -d '{"name": "Tim"}'
```

Your terminal will show logs similar to this:

```text
2025-12-01 17:31:43,276 | INFO  | Created endpoint: s8w0gpafvl6707 - hello-world-gpu-fb
2025-12-01 17:31:44,040 | INFO  | LiveServerless:s8w0gpafvl6707 | Started Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2
2025-12-01 17:31:44,090 | INFO  | Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2 | Status: IN_QUEUE
2025-12-01 17:31:44,479 | INFO  | Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2 | .
2025-12-01 17:31:45,757 | INFO  | Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2 | ..
...
2025-12-01 17:33:38,591 | INFO  | Job:8406eeeb-d174-4c36-867a-0e2a53e2084b-e2 | Status: COMPLETED
2025-12-01 17:33:38,618 | INFO  | Worker:na8dbgu8htou5e | Delay Time: 109571 ms
2025-12-01 17:33:38,619 | INFO  | Worker:na8dbgu8htou5e | Execution Time: 910 ms
dependency_installer.py:35 Installing Python dependencies: ['torch']
INFO:     127.0.0.1:52355 - "POST /gpu/hello HTTP/1.1" 200 OK
```

> Note: The first request takes ~60-90 seconds because flash has to provision a GPU worker and install dependencies like `torch`. Subsequent requests are much faster (~1-2s) as long as the worker stays warm. The `idleTimeout` setting controls how long workers stay alive after completing work.

### What happens under the hood

When you send a request to `/gpu/hello` (via Swagger UI or curl), FastAPI calls the `hello` route, which calls `say_hello()` in `endpoint.py`. Because `say_hello` is decorated with `@remote`, flash does the following instead of running it locally:

1. Creates (or reuses) a serverless endpoint on Runpod for `hello-world-gpu`.
2. Serializes the function input (`name`) and submits a job to that endpoint.
3. Provisions a GPU worker if none is available yet — this is the cold start you see as dots in the logs.
4. Installs the dependencies you listed in `dependencies=[\"torch\"]` on that worker.
5. Executes `say_hello` on the GPU and captures the return value.
6. Sends the result back to your FastAPI app, which returns it as the HTTP response.

### Expected output

```json
{
  "greeting": "Hello Tim, from a GPU!",
  "gpu": "NVIDIA GeForce RTX 4090",
  "cuda_available": true
}
```

## Alpha status

Flash is in alpha. Things may change or break. I'll do my best to update this article when that happens.

- `flash run` is local only (production deployment coming soon)
- Endpoints persist in your Runpod account until manually deleted
- Local `.env` vars don't auto-forward to workers (use `env={}` in config)

Report issues on [GitHub](https://github.com/runpod/tetra-rp/issues).
